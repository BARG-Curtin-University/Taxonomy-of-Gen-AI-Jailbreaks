# Taxonomy of Gen-AI Jailbreaks

This repository contains the data and Python script used in the generation of our research paper titled "Taxonomy of Gen-AI Jailbreaks â€“ Insights into bypassing language model safeguards." 

## Overview
The goal of this research project was to analyze and categorize various techniques used to bypass language model safeguards within the context of Generative AI (Gen-AI) systems. By exploring potential vulnerabilities and weaknesses in these models, we aim to shed light on the importance of robust safeguards in preventing unintended or malicious outputs. 

## Contents

- **data:** This directory includes the datasets used for training and evaluation of the Gen-AI models in our research. It consists of labeled examples, benchmark datasets, and relevant supplementary data.
- **scripts:** The Python script provided in this repository outlines the experimentation process and serves as a guideline to reproduce our research findings. It includes data preprocessing, model training, evaluation metrics, and sample code for analyzing the outputs.
- **results:** This folder contains the results obtained from our experiments, along with visualizations, tables, and other pertinent materials used in generating the research paper.

## Contribution
We encourage collaboration and contributions to further enhance our understanding of language model safeguards and discovering potential vulnerabilities. If you find any useful additions or improvements, please feel free to create a pull request or submit an issue.

**Note:** Please make sure to comply with ethical guidelines and use this research and code responsibly.

## Citation
If you find this research or code helpful, we kindly request that you cite our paper. The citation details can be found in the accompanying research paper or the "CITATION.md" file in this repository.

We hope that this research contributes to the ongoing discussions around the development, use, and safety of Generative AI systems.
